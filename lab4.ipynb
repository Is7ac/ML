{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVD24F+ejC0Pmdh3DpIc4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Is7ac/ML/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aaLrj8Fa2Op-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "X = np.array([[25, 1], [20, 0], [25, 1], [45, 2], [20, 1], [25, 2]])  # Age, Car (0: Vintage, 1: Sports, 2: SUV)\n",
        "y = np.array(['L', 'H', 'L', 'H', 'H', 'H'])  # Risk (H: High, L: Low)"
      ],
      "metadata": {
        "id": "rYVkEHXw4cJd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate entropy\n",
        "def entropy(y):\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy\n",
        "\n",
        "# Function to calculate information gain\n",
        "def information_gain(X, y, feature_index, split_value):\n",
        "    # Split data based on the feature and split value\n",
        "    X_left = X[X[:, feature_index] <= split_value]\n",
        "    X_right = X[X[:, feature_index] > split_value]\n",
        "    y_left = y[X[:, feature_index] <= split_value]\n",
        "    y_right = y[X[:, feature_index] > split_value]\n",
        "\n",
        "    # Calculate entropy for the subsets\n",
        "    entropy_parent = entropy(y)\n",
        "    entropy_left = entropy(y_left)\n",
        "    entropy_right = entropy(y_right)\n",
        "\n",
        "    # Calculate information gain\n",
        "    total_instances = len(y)\n",
        "    instances_left = len(y_left)\n",
        "    instances_right = len(y_right)\n",
        "    information_gain = entropy_parent - ((instances_left / total_instances) * entropy_left + (instances_right / total_instances) * entropy_right)\n",
        "\n",
        "    return information_gain\n",
        "\n"
      ],
      "metadata": {
        "id": "tK508xyC3YZ6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the information gain for each feature\n",
        "information_gains = []\n",
        "for feature_index in range(X.shape[1]):\n",
        "    unique_values = np.unique(X[:, feature_index])\n",
        "    for value in unique_values:\n",
        "        gain = information_gain(X, y, feature_index, value)\n",
        "        information_gains.append((feature_index, value, gain))\n",
        "\n",
        "# Select the feature with the highest information gain\n",
        "best_split = max(information_gains, key=lambda x: x[2])\n",
        "best_feature_index, best_split_value, best_gain = best_split\n",
        "\n",
        "# Construct the decision tree\n",
        "decision_tree = {\n",
        "    'feature_index': best_feature_index,\n",
        "    'split_value': best_split_value,\n",
        "    'left': {'class': 'L'},  # Low risk\n",
        "    'right': {'class': 'H'}  # High risk\n",
        "}\n",
        "\n",
        "# Classify the point (Age=27, Car=Vintage)\n",
        "def classify(point, tree):\n",
        "    if point[tree['feature_index']] <= tree['split_value']:\n",
        "        if 'class' in tree['left']:\n",
        "            return tree['left']['class']\n",
        "        else:\n",
        "            return classify(point, tree['left'])\n",
        "    else:\n",
        "        if 'class' in tree['right']:\n",
        "            return tree['right']['class']\n",
        "        else:\n",
        "            return classify(point, tree['right'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zLeSiuhh2RaA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test classification for the given point\n",
        "point_to_classify = np.array([27, 0])  # Age=27, Car=Vintage\n",
        "classification = classify(point_to_classify, decision_tree)\n",
        "print(\"Classification for point (Age=27, Car=Vintage):\", classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Fhe8SZ4rWw",
        "outputId": "ca88f4c3-3cea-48f9-b039-57647204293f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification for point (Age=27, Car=Vintage): H\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. True: High entropy means uncertainty or disorder in the dataset, which implies that the partitions in classification are not pure. When entropy is high, it indicates that there is a mix of different classes in the dataset, thus the partitions are not pure.\n",
        "b. False: A low Gini index does not necessarily mean that a node or leaf is pure. The Gini index measures the impurity of a node in a decision tree. While a low Gini index suggests less impurity, it does not guarantee purity, especially if there are multiple classes present in the node. A Gini index of 0 would indicate perfect purity, but it's not the only factor to consider."
      ],
      "metadata": {
        "id": "UC3vCMjs2Wxi"
      }
    }
  ]
}